import cv2
import time
import argparse
import os
import math

import mediapipe as mp

# ---------- Camera helpers (USB / CSI) ----------
def gstreamer_pipeline(
    capture_width=1280, capture_height=720,
    display_width=1280, display_height=720,
    framerate=30, flip_method=0,
):
    return (
        "nvarguscamerasrc ! "
        "video/x-raw(memory:NVMM), "
        f"width=(int){capture_width}, height=(int){capture_height}, "
        "format=(string)NV12, "
        f"framerate=(fraction){framerate}/1 ! "
        f"nvvidconv flip-method={flip_method} ! "
        f"video/x-raw, width=(int){display_width}, height=(int){display_height}, format=(string)BGRx ! "
        "videoconvert ! "
        "video/x-raw, format=(string)BGR ! appsink"
    )

def open_capture(use_csi, width, height, fps, device_index):
    if use_csi:
        return cv2.VideoCapture(
            gstreamer_pipeline(width, height, width, height, fps, 0),
            cv2.CAP_GSTREAMER
        )
    cap = cv2.VideoCapture(device_index)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
    cap.set(cv2.CAP_PROP_FPS, fps)
    return cap

# ---------- Drawing & FPS ----------
def draw_fps(img, fps):
    cv2.putText(img, f"FPS: {fps:.2f}", (10,30),
                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2, cv2.LINE_AA)

# ---------- Geometry helpers ----------
def dist(a, b):
    return math.hypot(a.x - b.x, a.y - b.y)

def is_thumb_extended(landmarks, handedness_label):
    # é€éæ‹‡æŒ‡å°–èˆ‡ IP/MCP çš„ç›¸å° x ä½ç½® + èˆ‡æŒå¿ƒè·é›¢ï¼Œåˆ¤æ–·æ˜¯å¦ä¼¸ç›´
    # å³æ‰‹ï¼šæ‹‡æŒ‡å°–(4) x < æ‹‡æŒ‡IP(3) x < æ‹‡æŒ‡MCP(2) x ä»£è¡¨å‘å³å¼µé–‹ï¼ˆé¡é ­æœªé¡åƒï¼‰
    # å·¦æ‰‹ï¼šåä¹‹
    tip = landmarks[4]
    ip  = landmarks[3]
    mcp = landmarks[2]
    if handedness_label == "Right":
        return tip.x < ip.x < mcp.x
    else:
        return tip.x > ip.x > mcp.x

def is_finger_extended(landmarks, tip_id):
    # å°é£ŸæŒ‡~å°æŒ‡ï¼štip.y < pip.y è¦–ç‚ºä¼¸ç›´ï¼ˆå½±åƒåº§æ¨™ y å‘ä¸‹ï¼‰
    pip_id = tip_id - 2
    return landmarks[tip_id].y < landmarks[pip_id].y

def count_fingers(landmarks, handedness_label):
    # è¨ˆç®—ä¼¸å‡ºçš„æ‰‹æŒ‡æ•¸ï¼šæ‹‡æŒ‡ + (é£Ÿ/ä¸­/ç„¡å/å°æŒ‡)
    count = 0
    if is_thumb_extended(landmarks, handedness_label):
        count += 1
    for tip in [8, 12, 16, 20]:
        if is_finger_extended(landmarks, tip):
            count += 1
    return count

def is_thumbs_up(landmarks, handedness_label):
    # æ‹‡æŒ‡ä¼¸ç›´ã€å…¶ä»–å››æŒ‡å½æ›²ã€ä¸”æ‹‡æŒ‡æ–¹å‘å¤§è‡´ä¸Šã€Œä¸Šä¸‹ã€è€Œéæ°´å¹³
    thumb_ok = is_thumb_extended(landmarks, handedness_label)
    others_folded = all(not is_finger_extended(landmarks, t) for t in [8,12,16,20])

    # æ‹‡æŒ‡å°–èˆ‡æŒ‡æ ¹çš„ y å·®è·è¦æ˜é¡¯
    tip = landmarks[4]
    mcp = landmarks[2]
    vertical = abs(tip.y - mcp.y) > abs(tip.x - mcp.x)  # è¿‘ä¼¼å‚ç›´
    return thumb_ok and others_folded and vertical

def is_peace(landmarks):
    # é£ŸæŒ‡ã€ä¸­æŒ‡ä¼¸ç›´ï¼Œç„¡åã€å°æŒ‡å½æ›²
    return (is_finger_extended(landmarks, 8) and
            is_finger_extended(landmarks, 12) and
            (not is_finger_extended(landmarks, 16)) and
            (not is_finger_extended(landmarks, 20)))

def is_ok_gesture(landmarks):
    # OKï¼šæ‹‡æŒ‡å°–(4) èˆ‡ é£ŸæŒ‡å°–(8) è·é›¢å¾ˆè¿‘ï¼Œä¸”ä¸­ã€ç„¡åã€å°æŒ‡å¤§å¤šä¼¸ç›´æˆ–è‡ªç„¶
    t = landmarks[4]
    i = landmarks[8]
    # è·é›¢ç”¨ã€Œç›¸å°æ‰‹æŒå¤§å°ã€æ­£è¦åŒ–ï¼šä»¥æ‰‹æŒï¼ˆ0 wrist åˆ°ä¸­æŒ‡ MCP(9)ï¼‰è·é›¢ç‚ºå°ºåº¦
    palm_scale = dist(landmarks[0], landmarks[9]) + 1e-6
    close = dist(t, i) / palm_scale < 0.25
    return close

def classify_gesture(landmarks, handedness_label):
    # å›å‚³ (gesture_name, extra_text)
    if is_thumbs_up(landmarks, handedness_label):
        return ("THUMBS_UP", "ğŸ‘")
    if is_ok_gesture(landmarks):
        return ("OK", "ğŸ‘Œ")
    if is_peace(landmarks):
        return ("PEACE", "âœŒï¸")
    cnt = count_fingers(landmarks, handedness_label)
    return (f"{cnt}_FINGERS", f"{cnt}")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csi", action="store_true", help="ä½¿ç”¨ CSI ç›¸æ©Ÿï¼ˆé è¨­ USBï¼‰")
    ap.add_argument("--device", type=int, default=0, help="USB æ”å½±æ©Ÿç´¢å¼•")
    ap.add_argument("--cam_w", type=int, default=1280)
    ap.add_argument("--cam_h", type=int, default=720)
    ap.add_argument("--cam_fps", type=int, default=30)
    ap.add_argument("--max_hands", type=int, default=2)
    ap.add_argument("--min_det", type=float, default=0.5)
    ap.add_argument("--min_track", type=float, default=0.5)
    ap.add_argument("--mirror", action="store_true", help="é¡åƒé¡¯ç¤ºï¼ˆåƒè‡ªæ‹ï¼‰")
    args = ap.parse_args()

    cap = open_capture(args.csi, args.cam_w, args.cam_h, args.cam_fps, args.device)
    if not cap.isOpened():
        print("ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ"); return

    mp_hands = mp.solutions.hands
    mp_draw  = mp.solutions.drawing_utils
    mp_style = mp.solutions.drawing_styles

    prev_t = time.time()
    fps_ema = 0.0
    alpha = 0.9

    # æ³¨æ„ï¼šMediaPipe Hands ä¸»è¦è·‘ CPUï¼Œä½†åœ¨ Jetson 640x480 ä¹Ÿèƒ½æ‹¿åˆ°ä¸éŒ¯ FPS
    with mp_hands.Hands(
        max_num_hands=args.max_hands,
        model_complexity=1,            # 0 è¼ƒå¿« / 1 å¹³è¡¡ / 2 è¼ƒæº–
        min_detection_confidence=args.min_det,
        min_tracking_confidence=args.min_track
    ) as hands:

        try:
            while True:
                ok, frame = cap.read()
                if not ok:
                    break

                if args.mirror:
                    frame = cv2.flip(frame, 1)

                # BGR -> RGB
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # MediaPipe è¦æ±‚ä¸å¯å¯«ä»¥åŠ é€Ÿ
                rgb.flags.writeable = False
                res = hands.process(rgb)

                # ç¹ªè£½çµæœ
                if res.multi_hand_landmarks:
                    for hand_lms, handedness in zip(res.multi_hand_landmarks, res.multi_handedness):
                        # ç•«éª¨æ¶
                        mp_draw.draw_landmarks(
                            frame, hand_lms, mp_hands.HAND_CONNECTIONS,
                            mp_style.get_default_hand_landmarks_style(),
                            mp_style.get_default_hand_connections_style()
                        )

                        # æ‰‹å‹¢åˆ¤æ–·
                        label = handedness.classification[0].label  # "Left"/"Right"
                        g_name, g_text = classify_gesture(hand_lms.landmark, label)

                        # é¡¯ç¤ºæ–‡å­—
                        x0 = int(hand_lms.landmark[0].x * frame.shape[1])
                        y0 = int(hand_lms.landmark[0].y * frame.shape[0]) - 10
                        cv2.putText(frame, f"{label} {g_name} {g_text}",
                                    (x0, max(30, y0)),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)

                # FPS
                now = time.time()
                inst = 1.0 / max(1e-6, now - prev_t)
                prev_t = now
                fps_ema = inst if fps_ema == 0 else alpha*fps_ema + (1-alpha)*inst
                draw_fps(frame, fps_ema)

                cv2.imshow("MediaPipe Hands - Gestures", frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        finally:
            cap.release()
            cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
